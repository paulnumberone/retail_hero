{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uplift\n",
    "## Целиком ноутбук будет выполняться больше 2 недель, ГДЕ пропускать ОТМЕЧЕНО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import lightgbm as lgbm\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm \n",
    "\n",
    "start=datetime.datetime.now()\n",
    "\n",
    "def uplift_fit_predict(model, X_train, treatment_train, target_train, X_test):\n",
    "    \"\"\"\n",
    "    Реализация простого способа построения uplift-модели.\n",
    "    \n",
    "    Обучаем два одинаковых бинарных классификатора, которые оценивают вероятность target для клиента:\n",
    "    1. с которым была произведена коммуникация (treatment=1)\n",
    "    2. с которым не было коммуникации (treatment=0)\n",
    "    \n",
    "    В качестве оценки uplift для нового клиента берется разница оценок вероятностей:\n",
    "    Predicted Uplift = P(target|treatment=1) - P(target|treatment=0)\n",
    "    \"\"\"\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "    model_treatment = clone(model).fit(X_treatment, y_treatment)\n",
    "    model_control = clone(model).fit(X_control, y_control)\n",
    "    predict_treatment = model_treatment.predict_proba(X_test)[:, 1]\n",
    "    predict_control = model_control.predict_proba(X_test)[:, 1]\n",
    "    predict_uplift = predict_treatment - predict_control\n",
    "    return predict_uplift\n",
    "\n",
    "def uplift_fit_predict_2_models(model_treatment, model_control, X_train, treatment_train, target_train, X_test):\n",
    "    \"\"\"\n",
    "    Обучаем два разных бинарных классификатора, которые оценивают вероятность target для клиента:\n",
    "    1. с которым была произведена коммуникация (treatment=1)\n",
    "    2. с которым не было коммуникации (treatment=0)\n",
    "\n",
    "    \"\"\"\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "    model_treatment.fit(X_treatment, y_treatment)\n",
    "    model_control.fit(X_control, y_control)\n",
    "    predict_treatment = model_treatment.predict_proba(X_test)[:, 1]\n",
    "    predict_control = model_control.predict_proba(X_test)[:, 1]\n",
    "    predict_uplift = predict_treatment - predict_control\n",
    "    return predict_uplift\n",
    "\n",
    "def max_absolute_value_2(dataframe):\n",
    "    \"\"\"\n",
    "    Выбор из двух серий того значения с одинаковым индексом, которое больше по модулю\n",
    "\n",
    "    \"\"\"\n",
    "    buff = []\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "        if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 0]):\n",
    "            buff.append(dataframe.iloc[i, 1])\n",
    "        else:\n",
    "            buff.append(dataframe.iloc[i, 0])\n",
    "    return pd.DataFrame({'uplift': list(buff)}) \n",
    "\n",
    "def max_absolute_value_3(dataframe):\n",
    "    \"\"\"\n",
    "    Выбор из трех серий того значения с одинаковым индексом, которое больше по модулю\n",
    "\n",
    "    \"\"\"\n",
    "    buff = []\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "        if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 0]):\n",
    "            if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 2]):\n",
    "                buff.append(dataframe.iloc[i, 1])\n",
    "            else:\n",
    "                buff.append(dataframe.iloc[i, 2])\n",
    "        else:\n",
    "            if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 2]):\n",
    "                buff.append(dataframe.iloc[i, 0])\n",
    "            else:\n",
    "                buff.append(dataframe.iloc[i, 2])\n",
    "    return buff\n",
    "\n",
    "def max_absolute_value_4(dataframe):\n",
    "    \"\"\"\n",
    "    Выбор из 4ч серий того значения с одинаковым индексом, которое больше по модулю\n",
    "\n",
    "    \"\"\"\n",
    "    buff = []\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "        if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 0]):\n",
    "            if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 2]):\n",
    "                if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 1])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "            else:\n",
    "                if abs(dataframe.iloc[i, 2]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 2])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "        else:\n",
    "            if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 2]):\n",
    "                if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 0])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "            else:\n",
    "                if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 2])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "    return buff\n",
    "\n",
    "def uplift_fit_predict_2_concurrent_models(model_treatment_1, model_treatment_2, model_control_1, model_control_2, metric,\n",
    "                                           X_train, treatment_train, target_train, X_test):\n",
    "    \"\"\"\n",
    "    Обучение для каждой группы клиентов по 2 классификатора, предсказание которых определяется по метрике:\n",
    "    mean - из двух предсказаний построчно выбирается их среднее\n",
    "    max - из двух предсказаний построчно выбирается их максимум\n",
    "    min - из двух предсказаний построчно выбирается их минимум\n",
    "    max_abs - из двух предсказаний построчно выбирается то, которое больше по модулю, с преним знаком\n",
    "\n",
    "    \"\"\"\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "    model_treatment_1.fit(X_treatment, y_treatment)\n",
    "    model_treatment_2.fit(X_treatment, y_treatment)\n",
    "    model_control_1.fit(X_treatment, y_treatment)\n",
    "    model_control_2.fit(X_control, y_control)\n",
    "    \n",
    "    if metric == 'mean':\n",
    "        predict_treatment = pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1).mean(axis=1)\n",
    "\n",
    "        predict_control = pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1).mean(axis=1)\n",
    "    if metric == 'max':\n",
    "        predict_treatment = pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1).max(axis=1)\n",
    "\n",
    "        predict_control = pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1).max(axis=1)\n",
    "        \n",
    "    if metric == 'min':\n",
    "        predict_treatment = pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1).min(axis=1)\n",
    "\n",
    "        predict_control = pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1).min(axis=1)\n",
    "        \n",
    "    if metric == 'max_abs':\n",
    "        predict_treatment = max_absolute_value_2(pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1))\n",
    "\n",
    "        predict_control = max_absolute_value_2(pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1))\n",
    "    \n",
    "    predict_uplift = predict_treatment - predict_control\n",
    "    return predict_uplift\n",
    "\n",
    "def uplift_score(prediction, treatment, target, rate=0.3):\n",
    "    \"\"\"\n",
    "    Подсчет Uplift Score\n",
    "    \"\"\"\n",
    "    order = np.argsort(-prediction)\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "    score = treatment_p - control_p\n",
    "    return score\n",
    "\n",
    "def important_feats_for_model(model, features):\n",
    "    \"\"\"\n",
    "    Определение наиболее важных признаков для модели\n",
    "    \"\"\"\n",
    "    X_train=features.loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0], target_train[treatment_train == 0]\n",
    "    model_treatment = clone(model).fit(X_treatment, y_treatment)\n",
    "    model_control = clone(model).fit(X_control, y_control)\n",
    "    print(pd.DataFrame(model_treatment.feature_importances_, features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных\n",
    "df_train = pd.read_csv('data/uplift_train.csv', index_col='client_id')\n",
    "df_test = pd.read_csv('data/uplift_test.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МОЖНО ПРОПУСТИТЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляю данные о покупках и товарах\n",
    "df_products = pd.read_csv('data/products.csv')\n",
    "df_purchases = pd.read_csv('data/purchases.csv')\n",
    "df_clients = pd.read_csv('data/clients.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение признаков\n",
    "df_clients['first_redeem_unixtime'] = pd.Series([y.timestamp() for y in \n",
    "                                                 [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "                                                  df_clients['first_issue_date']]]).values\n",
    "df_clients['first_issue_unixtime'] = pd.Series([x.timestamp() for x in \n",
    "                                                [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "                                                 df_clients['first_redeem_date'].fillna('1990-01-01 01:01:01')]]).values\n",
    "df_clients['issue_redeem_delay'] = df_clients['first_redeem_unixtime'] - df_clients['first_issue_unixtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = ['regular_points_received', 'express_points_received','regular_points_spent', 'express_points_spent', 'purchase_sum','store_id']\n",
    "all_hist = df_purchases.groupby(['client_id','transaction_id'])[last_cols].last()\n",
    "last_month = df_purchases[df_purchases['transaction_datetime'] > '2019-02-18'].groupby(['client_id','transaction_id'])[last_cols].last()\n",
    "\n",
    "features =  pd.concat([all_hist.groupby('client_id')['purchase_sum'].count(),\n",
    "                       last_month.groupby('client_id')['purchase_sum'].count(),\n",
    "                       all_hist.groupby('client_id').sum(),\n",
    "                       all_hist.groupby('client_id')[['store_id']].nunique(),\n",
    "                       last_month.groupby('client_id').sum(),\n",
    "                       last_month.groupby('client_id')[['store_id']].nunique(),\n",
    "                      ],axis = 1)\n",
    "features.columns = ['total_trans_count','last_month_trans_count']+list(c+\"_sum_all\" for c in last_cols)+list(c+\"_sum_last_month\" for c in last_cols)\n",
    "df_clients[list(features.columns)] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_clients = pd.read_csv('data/clients.csv', index_col='client_id', parse_dates=['first_issue_date','first_redeem_date'])\n",
    "df_clients['first_issue_date_weekday'] = temp_df_clients['first_issue_date'].dt.weekday\n",
    "df_clients['first_redeem_date_weekday'] = temp_df_clients['first_redeem_date'].dt.weekday\n",
    "df_clients['first_issue_date_hour'] = temp_df_clients['first_issue_date'].dt.hour\n",
    "df_clients['first_redeem_date_hour'] = temp_df_clients['first_redeem_date'].dt.hour\n",
    "del temp_df_clients, all_hist, last_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Часы, в которые клиенты совершали покупки\n",
    "df_purchases['transaction_hour'] = pd.Series([x.hour for x in \n",
    "           [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "            df_purchases['transaction_datetime']]])\n",
    "df_temp = df_purchases[['client_id','transaction_hour']].groupby(['client_id','transaction_hour']).nunique()\n",
    "users, hours, nums = [], [], []\n",
    "for i in range(0, len(df_temp.index)):\n",
    "    users.append(df_temp.index[i][0])\n",
    "    hours.append(df_temp.index[i][1])\n",
    "    nums.append(df_temp.values[i][0])\n",
    "boughts_by_hours = pd.DataFrame(data=zip(users, hours, nums), columns=['client_id', 'hours', 'nums'])\n",
    "boughts_by_hours_dum = pd.get_dummies(boughts_by_hours['hours'])\n",
    "boughts_by_hours_dum['client_id'] = boughts_by_hours['client_id']\n",
    "boughts_by_hours_dum = boughts_by_hours_dum.groupby(['client_id']).sum().astype(int)\n",
    "boughts_by_hours_dum.columns = ['0h', '1h', '2h', '3h', '4h', '5h', '6h', '7h', '8h', '9h', '10h', '11h', '12h', '13h', \n",
    "                                '14h', '15h', '16h', '17h', '18h', '19h', '20h', '21h', '22h', '23h']\n",
    "df_clients = pd.concat([df_clients, boughts_by_hours_dum], join='outer', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дни, в которые клиенты совершали покупки\n",
    "\n",
    "df_purchases['transaction_day'] = pd.Series([x.weekday() for x in \n",
    "           [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "            df_purchases['transaction_datetime']]])\n",
    "df_temp = df_purchases[['client_id','transaction_day']].groupby(['client_id','transaction_day']).nunique()\n",
    "users, days, nums = [], [], []\n",
    "for i in range(0, len(df_temp.index)):\n",
    "    users.append(df_temp.index[i][0])\n",
    "    days.append(df_temp.index[i][1])\n",
    "    nums.append(df_temp.values[i][0])\n",
    "boughts_by_days = pd.DataFrame(data=zip(users, days, nums), columns=['client_id', 'days', 'nums'])\n",
    "boughts_by_days_dum = pd.get_dummies(boughts_by_days['days'])\n",
    "boughts_by_days_dum['client_id'] = boughts_by_days['client_id']\n",
    "boughts_by_days_dum = boughts_by_days_dum.groupby(['client_id']).sum().astype('category')\n",
    "boughts_by_days_dum.columns = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "df_clients = pd.concat([df_clients, boughts_by_days_dum], join='outer', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del users, hours, days, nums, boughts_by_hours, boughts_by_hours_dum, boughts_by_days, boughts_by_days_dum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конверт дат в юникс\n",
    "dt_list = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in df_purchases['transaction_datetime']]\n",
    "df_purchases['transaction_datetime'] = pd.Series([y.timestamp() for y in dt_list]).values\n",
    "del dt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пол в категорию\n",
    "df_clients['gender_M'] = (df_clients['gender'] == 'M').astype('category')\n",
    "df_clients['gender_F'] = (df_clients['gender'] == 'F').astype('category')\n",
    "df_clients['gender_U'] = (df_clients['gender'] == 'U').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество товаров куплено клиентом\n",
    "df_clients['qty_items'] = df_purchases[['client_id', 'transaction_id']].groupby(['client_id']).count()\n",
    "# Потрачено клиентом\n",
    "df_clients['amount'] = df_purchases[['client_id', 'purchase_sum']].groupby(['client_id']).sum()\n",
    "# Количество покупок клиентом \n",
    "df_clients['n_purchases'] = df_purchases.groupby(['client_id']).transaction_id.nunique()\n",
    "# Количество магазинов, посещенных клиентом \n",
    "df_clients['n_shops'] = df_purchases.groupby(['client_id']).store_id.nunique()\n",
    "# Максимальная сумма чека клиента\n",
    "df_clients['max_purch_sum'] = df_purchases.groupby(['client_id']).purchase_sum.max()\n",
    "# Минимальная сумма чека клиента\n",
    "df_clients['min_purch_sum'] = df_purchases.groupby(['client_id']).purchase_sum.min().fillna(1).replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наличие алкоголя в чеке\n",
    "df_clients['has_alco'] = (pd.merge(df_purchases, df_products[['product_id', 'is_alcohol']], on = 'product_id', \n",
    "                                   sort=False).groupby(['client_id']).is_alcohol.sum()>0).astype('category')\n",
    "# Средний чек покупателя\n",
    "df_clients['avg_check'] = df_clients['amount'] / df_clients['n_purchases']\n",
    "# Периодичность покупок\n",
    "df_clients['purch_freq'] = (df_purchases.groupby(['client_id']).transaction_datetime.max() - \n",
    "                            df_purchases.groupby(['client_id']).transaction_datetime.min())/df_clients['n_purchases']\n",
    "# Наличие алкоголя и товаров своей марки в чеке\n",
    "df_clients['has_alco_and_own'] = ((pd.merge(df_purchases, df_products[['product_id', 'is_alcohol']], on = 'product_id', \n",
    "                                   sort=False).groupby(['client_id']).is_alcohol.sum()>0) & \n",
    "                                 (pd.merge(df_purchases, df_products[['product_id', 'is_own_trademark']], on = 'product_id', \n",
    "                                   sort=False).groupby(['client_id']).is_own_trademark.mean()>0.5)).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возраст покупателя\n",
    "df_clients['age_error'] = ((df_clients['age'] <= 10) & (df_clients['age'] > 99)).astype('category')\n",
    "df_clients['age_1'] = ((df_clients['age'] >10) & (df_clients['age'] <= 18)).astype('category')\n",
    "df_clients['age_2'] = ((df_clients['age'] >18) & (df_clients['age'] <= 25)).astype('category')\n",
    "df_clients['age_3'] = ((df_clients['age'] >25) & (df_clients['age'] <= 35)).astype('category')\n",
    "df_clients['age_4'] = ((df_clients['age'] >35) & (df_clients['age'] <= 45)).astype('category')\n",
    "df_clients['age_5'] = ((df_clients['age'] >45) & (df_clients['age'] <= 55)).astype('category')\n",
    "df_clients['age_6'] = ((df_clients['age'] >55) & (df_clients['age'] <= 99)).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Категирии по суммам и количеству купленных наименований\n",
    "df_clients['client_cat_1'] = (df_clients['qty_items'] <= 45).astype('category')\n",
    "df_clients['client_cat_2'] = ((df_clients['qty_items'] >45) & (df_clients['qty_items'] <= 85)).astype('category')\n",
    "df_clients['client_cat_3'] = ((df_clients['qty_items'] >85) & (df_clients['qty_items'] <= 155)).astype('category')\n",
    "df_clients['client_cat_4'] = (df_clients['qty_items'] > 155).astype('category')\n",
    "\n",
    "df_clients['client_sum_1'] = (df_clients['min_purch_sum'] <= 50).astype('category')\n",
    "df_clients['client_sum_2'] = ((df_clients['min_purch_sum'] >50) & (df_clients['min_purch_sum'] <= 150)).astype('category')\n",
    "df_clients['client_sum_3'] = ((df_clients['min_purch_sum'] >150) & (df_clients['min_purch_sum'] <= 900)).astype('category')\n",
    "df_clients['client_sum_4'] = (df_clients['min_purch_sum'] > 900).astype('category')\n",
    "df_clients['client_sum_extra'] = ((df_clients['min_purch_sum'] > 1500) & (df_clients['max_purch_sum'] >15000)).astype('category')\n",
    "\n",
    "df_clients['sums_relation'] = df_clients['max_purch_sum'] / df_clients['min_purch_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Любимые и нелюбимые категории товаров\n",
    "little_df = pd.merge(df_purchases[['client_id', 'product_id']], df_products[['product_id', 'segment_id']], \n",
    "                     on = 'product_id', sort=True).groupby(['client_id', 'segment_id']).count().reset_index()\n",
    "little_df = little_df.sort_values(by = ['client_id', 'product_id'],ascending=False)\n",
    "little_df_max = little_df.groupby(['client_id']).max().to_dict()\n",
    "little_df_min = little_df.groupby(['client_id']).min().to_dict()\n",
    "\n",
    "little_df_max = pd.DataFrame.from_dict(little_df_max, orient='index').transpose()\n",
    "little_df_min = pd.DataFrame.from_dict(little_df_min, orient='index').transpose()\n",
    "\n",
    "df_clients['client_fav_cat'] = little_df_max.segment_id\n",
    "df_clients['client_unfav_cat'] = little_df_min.segment_id\n",
    "del little_df, little_df_max, little_df_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['regular_points_received'] = df_purchases[['client_id', 'transaction_id', 'regular_points_received']].groupby(['client_id']).sum()\n",
    "df_clients['regular_points_spent'] = df_purchases[['client_id', 'transaction_id', 'regular_points_spent']].groupby(['client_id']).sum().replace(0, 1)\n",
    "df_clients['regular_points_balance'] = df_clients['regular_points_received'] + df_clients['regular_points_spent']\n",
    "df_clients['bonuses'] = round(df_clients['amount']/df_clients['regular_points_spent'], 2)\n",
    "df_clients['max_product_quantity'] = df_purchases[['client_id', 'product_quantity']].groupby(['client_id']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['express_points_received'] = df_purchases[['client_id', 'transaction_id', 'express_points_received']].groupby(['client_id']).sum()\n",
    "df_clients['express_points_spent'] = df_purchases[['client_id', 'transaction_id', 'express_points_spent']].groupby(['client_id']).sum().replace(0, 1)\n",
    "df_clients['express_points_balance'] = df_clients['express_points_received'] + df_clients['express_points_spent']\n",
    "df_clients['express_bonuses'] = round(df_clients['express_points_spent']/df_clients['amount'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможные социальные категории\n",
    "df_clients['possible_pension']=((((df_clients['7h'])+(df_clients['8h'])+(df_clients['9h'])+(df_clients['10h'])+\n",
    "                                  (df_clients['11h'])+(df_clients['12h']))>0) & (df_clients['age']>60)).astype('category')\n",
    "df_clients['possible_worker']=(((df_clients['17h']+df_clients['18h']+df_clients['19h']+df_clients['20h']+\n",
    "                                 df_clients['21h'])>0) & (df_clients['age']>16)).astype('category')\n",
    "df_clients['possible_hard_worker']=(((df_clients['1h']+df_clients['2h']+df_clients['3h']+df_clients['23h']+\n",
    "                                      df_clients['22h']+df_clients['21h'])>0) & (df_clients['age']>20) & \n",
    "                                    (df_clients['has_alco'].astype(int))>0).astype('category')\n",
    "df_clients['shops_for_weekend'] = (((df_clients['fri'].astype(int)+df_clients['sat'].astype(int))>1) & \n",
    "                                   df_clients['has_alco'].astype(int)>0).astype('category')\n",
    "df_clients['shops_for_week'] = ((df_clients['fri'].astype(int)+df_clients['sat'].astype(int)+\n",
    "                                 df_clients['sun'].astype(int))==1).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормирую время\n",
    "df_clients['norm_first_issue_unixtime'] = df_clients['first_issue_unixtime']/df_clients['first_issue_unixtime'].values.mean()\n",
    "df_clients['norm_first_redeem_unixtime'] = df_clients['first_redeem_unixtime']/df_clients['first_redeem_unixtime'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потрачено клиентом нормированное\n",
    "df_clients['norm_amount'] = df_clients['amount']/df_clients['amount'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество магазинов, посещенных клиентом и чек нормированные\n",
    "df_clients['norm_n_shops'] = np.log(df_clients['n_shops'])\n",
    "df_clients['norm_avg_check'] = np.log(df_clients['avg_check'])\n",
    "df_clients['norm_qty_items'] = np.log(df_clients['qty_items'])\n",
    "df_clients['norm_min_purch_sum'] = np.log(df_clients['min_purch_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_operate = ['total_trans_count', 'last_month_trans_count', 'regular_points_received_sum_all', \n",
    "                       'express_points_received_sum_all', 'regular_points_spent_sum_all', 'express_points_spent_sum_all', \n",
    "                       'purchase_sum_sum_all', 'purchase_sum_sum_all', 'regular_points_received_sum_last_month', \n",
    "                       'express_points_received_sum_last_month', 'regular_points_spent_sum_last_month', \n",
    "                       'express_points_spent_sum_last_month', 'purchase_sum_sum_last_month', 'qty_items', \n",
    "                       'amount', 'n_purchases', 'max_purch_sum', 'min_purch_sum', 'avg_check', 'sums_relation', \n",
    "                       'regular_points_received', 'regular_points_spent', 'regular_points_balance', 'bonuses', \n",
    "                       'max_product_quantity', 'express_points_received', 'express_points_spent', 'express_points_balance',\n",
    "                       'express_bonuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(features_to_operate)):\n",
    "    for j in range(0, len(features_to_operate)):\n",
    "        if i!=j:\n",
    "            df_clients[features_to_operate[i] + '_sum_' + features_to_operate[j]] = (df_clients[features_to_operate[i]] + df_clients[features_to_operate[j]])\n",
    "            df_clients[features_to_operate[i] + '_multi_' + features_to_operate[j]] = (df_clients[features_to_operate[i]] * df_clients[features_to_operate[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=5802, microseconds=691259)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Экспорт на случай если все рухнет\n",
    "df_clients.to_csv('D:/df_clients.csv')\n",
    "datetime.datetime.now()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Освобождаю память\n",
    "del df_products\n",
    "del df_purchases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДО СЮДА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение после обвала\n",
    "df_clients = pd.read_csv('D:/df_clients.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалить нормированные признаки\n",
    "# features = list(df_clients.columns[2:-7].values) # удаляю 'first_issue_date', 'first_redeem_date'\n",
    "features = list(df_clients.columns[2:].values) # удаляю 'first_issue_date', 'first_redeem_date'\n",
    "#df_clients[features].fillna(0)\n",
    "features.remove('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбираем важные для модели фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отбираем важные для модели фичи\n",
    "import feature_selector\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "# Для treatment_flg\n",
    "fs = FeatureSelector(df_clients[features].loc[indices_learn, :].fillna(0), df_train.loc[indices_learn, 'treatment_flg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# признаки, имеющие более 60% пропусков\n",
    "fs.identify_missing(missing_threshold = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# доли незаполненных значений для каждого параметра\n",
    "fs.missing_stats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# признаки для удаления\n",
    "fs.ops['missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_collinear(correlation_threshold = 0.99)\n",
    "\n",
    "# признаки коллинеарные\n",
    "fs.record_collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "fs.plot_collinear(plot_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для target\n",
    "fs = FeatureSelector(df_clients[features].loc[indices_learn, :].fillna(0), df_train.loc[indices_learn, 'target'])\n",
    "# Oтбор признаков с нулевой важностью для target\n",
    "fs.identify_zero_importance(task = 'classification', \n",
    "                            eval_metric = 'auc', \n",
    "                            n_iterations = 10)\n",
    "\n",
    "# Hормализованные показатели важности plot_n самых значимых признаков\n",
    "fs.plot_feature_importances(threshold = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = list(fs.ops['zero_importance'])\n",
    "to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in to_delete:\n",
    "    features.remove(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследуем фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МОЖНО ПРОПУСТИТЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аномалий во взаимосвязях не выявлено\n",
    "for i in range(0, len(features)):\n",
    "    for j in range(0, len(features)):\n",
    "        plt.rcParams['figure.figsize'] = (10,8)\n",
    "        plt.scatter(df_clients[features[i]].loc[indices_train[:200000]], df_clients[features[j]].loc[indices_train[:200000]],\n",
    "               c='green', s=3)\n",
    "        plt.scatter(df_clients[features[i]].loc[indices_test[:200000]], df_clients[features[j]].loc[indices_test[:200000]],\n",
    "               c='red', s=2)\n",
    "        plt.ylabel(features[i])\n",
    "        plt.xlabel(features[j])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "for i in range(4, len(features)):\n",
    "    plt.rcParams['figure.figsize'] = (10,8)\n",
    "    plt.title('Распределение целевого признака ' + str(features[i]) + 'для групп покупателей')\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(df_clients[features[i]].loc[indices_train], color='green')\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(df_clients[features[i]].loc[indices_test], color='red')\n",
    "    plt.title('Распределение целевого признака ' + str(features[i]) + ' для групп покупателей')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение предсказания одним алгоритмом для 0 и 1\n",
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по подобранным n_estimators = 170, max_depth = 1\n",
    "\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=GradientBoostingClassifier(n_estimators = 170, max_depth = 1),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n",
    "\n",
    "#Max validation score: 0.08205685972385873 = # Public: 0,0908\n",
    "# 0.07510360333141991 = 0,0788\n",
    "# 0.0767923502220611 = 0,0837\n",
    "# 0.07512500377654918 = 0,0859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=GradientBoostingClassifier(n_estimators = 170, max_depth = 1),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission34.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по ADABOOST по подобранным learning_rate = 0.1, n_estimators = 100\n",
    "\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=AdaBoostClassifier(learning_rate = 0.1, n_estimators = 100, n_jobs=-1),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n",
    "\n",
    "# Max validation score: 0.08268628458060162 = # Public: 0,0803\n",
    "# 0.07839801303161226 = 0,0765\n",
    "# 0.07623342094927343 = 0,0767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=AdaBoostClassifier(learning_rate = 0.1, n_estimators = 100, n_jobs=-1),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission35.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по LGBM\n",
    "\n",
    "import lightgbm as lgbm\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=lgbm.LGBMClassifier(learning_rate = 0.03, max_depth = 5, num_leaves = 20,\n",
    "             min_data_in_leaf = 3, application = 'binary',\n",
    "             subsample = 0.8, colsample_bytree = 0.8,\n",
    "             reg_alpha = 0.01,data_random_seed = 42,metric = 'binary_logloss',\n",
    "             max_bin = 416,bagging_freq = 3,reg_lambda = 0.01, n_estimators = 1000),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n",
    "\n",
    "# Max validation score: 0.058409997280884585 = 0,0745\n",
    "# 0.058173962959605974 = 0,0819\n",
    "# 0.06286884296605144 = 0,0773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=lgbm.LGBMClassifier(learning_rate = 0.03, max_depth = 5, num_leaves = 20,\n",
    "             min_data_in_leaf = 3, application = 'binary',\n",
    "             subsample = 0.8, colsample_bytree = 0.8,\n",
    "             reg_alpha = 0.01,data_random_seed = 42,metric = 'binary_logloss',\n",
    "             max_bin = 416,bagging_freq = 3,reg_lambda = 0.01, n_estimators = 1000),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission36.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по XGBClassifier\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=XGBClassifier(),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n",
    "\n",
    "# 0.06186994571840043 = 0,0918\n",
    "# 0.06186994571840043 = \n",
    "# 0.06186994571840043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=XGBClassifier(),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission37.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор гиперпараметров для моделей по отдельности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "X_train=df_clients[features].loc[indices_learn, :].fillna(0).values\n",
    "treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_train=df_train.loc[indices_learn, 'target'].values\n",
    "X_test=df_clients[features].loc[indices_valid, :].fillna(0).values\n",
    "X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': range(1, 6), \n",
    "              'tree_method': ['auto', 'exact', 'approx'],# 'hist', 'gpu_hist'],\n",
    "              'updater': ['grow_colmaker', 'distcol', 'grow_histmaker'], # 'grow_local_histmaker', # 'grow_skmaker', 'grow_quantile_histmaker', 'sync', 'refresh'], \n",
    "              'eta': np.arange(0.01, 0.21, 0.02),\n",
    "              'num_parallel_tree': [1, 2, 3]}\n",
    "\n",
    "model=XGBClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 3, verbose=True)\n",
    "model_treatment_XGBC = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_XGBC = clf.fit(X_control, y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_treatment_XGBC.best_params_, model_treatment_XGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_XGBC.best_params_, model_control_XGBC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "X_train=df_clients[features].loc[indices_learn, :].fillna(0).values\n",
    "treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_train=df_train.loc[indices_learn, 'target'].values\n",
    "X_test=df_clients[features].loc[indices_valid, :].fillna(0).values\n",
    "X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': range(150, 250, 10),\n",
    "              'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "              'loss': ['deviance', 'exponential'],\n",
    "              'max_depth': range(1, 4), \n",
    "              'learning_rate': np.arange(0.05, 0.14, 0.02), \n",
    "              'min_samples_leaf':[1, 2]}\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv = 3, n_jobs = -1, verbose=True)\n",
    "model_treatment_gbc = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_gbc = clf.fit(X_control, y_control)\n",
    "# в каггле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_treatment_gbc.best_params_, model_treatment_gbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_control_gbc.best_params_, model_control_gbc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': range(40, 250, 10), \n",
    "              'learning_rate': np.arange(0.01, 0.21, 0.02)}\n",
    "\n",
    "model=AdaBoostClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 3, n_jobs = -1, verbose=True)\n",
    "model_treatment_adaboost = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_adaboost = clf.fit(X_control, y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_treatment_adaboost.best_params_, model_treatment_adaboost.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_adaboost.best_params_, model_control_adaboost.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "parameters = {'max_depth': range(1, 4), \n",
    "              'n_estimators': range(150, 250, 10),\n",
    "              'learning_rate': np.arange(0.01, 0.21, 0.02)}\n",
    "\n",
    "model=lgbm.LGBMClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 3, n_jobs = -1, verbose=True)\n",
    "model_treatment_LGBM = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_control_LGBM = clf.fit(X_control, y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_treatment_LGBM.best_params_, model_treatment_LGBM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_control_LGBM.best_params_, model_control_LGBM.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание по 2 моделям, разным для таргета и тритмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = features[:2000]\n",
    "#features = features[:94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "start=datetime.datetime.now()\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "models_treatment = [GradientBoostingClassifier(learning_rate=0.11000000000000001, max_depth=3, min_samples_leaf=1, n_estimators=170),\n",
    "                    lgbm.LGBMClassifier(learning_rate = 0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                    AdaBoostClassifier(learning_rate=0.18999999999999997, n_estimators=230),\n",
    "                    XGBClassifier(eta=0.01, max_depth=4)]\n",
    "\n",
    "models_control = [GradientBoostingClassifier(learning_rate=0.05, max_depth=3, min_samples_leaf=1, n_estimators=240),\n",
    "                  lgbm.LGBMClassifier(learning_rate=0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                  AdaBoostClassifier(learning_rate=0.10999999999999997, n_estimators=230),\n",
    "                  XGBClassifier(eta=0.01, max_depth=4)]\n",
    "\n",
    "from tqdm import tqdm \n",
    "i=0\n",
    "for model_treatment in tqdm(models_treatment):\n",
    "    for model_control in tqdm(models_control):\n",
    "        test_uplift = uplift_fit_predict_2_models(model_treatment, model_control,\n",
    "            X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "            treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "            target_train=df_train.loc[indices_train, 'target'].values,\n",
    "            X_test=df_clients[features].loc[indices_test, :].fillna(0).values)\n",
    "        \n",
    "        df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "        df_submission.to_csv('отправить/3(фичикатегории)/submission3000_'+str(i)+'.csv')\n",
    "        i += 1\n",
    "        \n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Все 16 решений отравлены, выбираю 4 лучших и комбинирую"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинация из 4 решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "godn_submission1 = pd.read_csv(r'отправить/отправлено/submission1.csv', index_col='client_id')\n",
    "godn_submission13 = pd.read_csv(r'отправить/отправлено/submission13.csv', index_col='client_id')\n",
    "godn_submission2_0 = pd.read_csv(r'отправить/отправлено/submission2_0.csv', index_col='client_id')\n",
    "godn_submission37 = pd.read_csv(r'отправить/отправлено/submission37.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13, godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:4].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:4].min(axis=1)\n",
    "effective_subs['abs_max'] = effective_subs.iloc[:, 0:4].abs().max(axis=1)\n",
    "effective_subs['abs_min'] = effective_subs.iloc[:, 0:4].abs().min(axis=1)\n",
    "for i in range(0, 5):\n",
    "    effective_subs.iloc[:, 4+i].to_csv(r'отправить/отправлено/submissions_compositions'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13, godn_submission2_0], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "effective_subs['abs_max'] = effective_subs.iloc[:, 0:3].abs().max(axis=1)\n",
    "effective_subs['abs_min'] = effective_subs.iloc[:, 0:3].abs().min(axis=1)\n",
    "for i in range(0, 5):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_2'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего зашли минимум, максимум и среднее по 4м лучшим решениям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинации из 3 решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_3'+str(i)+'.csv')\n",
    "    i+=1\n",
    "# 0,0912, 0,0861, 0,0928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_4'+str(i)+'.csv')\n",
    "    i+=1\n",
    "# 0,0941 0,0878 0,0927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission13, godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_5'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинации из 2 решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_6'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission2_0], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_7'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_8'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission13, godn_submission2_0], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_9'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission13, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_10'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_11'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 решения с лучшим паблик скором объединяю по абсолютному значению (скор не побил)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_1 = pd.read_csv(r'отправить/submissions_compositions0.csv', index_col='client_id')\n",
    "best_2 = pd.read_csv(r'отправить/submissions_compositions_102.csv', index_col='client_id')\n",
    "best_3 = pd.read_csv(r'отправить/submissions_compositions_111.csv', index_col='client_id')\n",
    "best_df = pd.concat([best_1, best_2, best_3], join='outer', axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df = pd.DataFrame({'uplift': list(max_absolute_value_3(best_df))}, index=best_1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df.to_csv(r'отправить/best_df.csv')\n",
    "# 0,0939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДО СЮДА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_1 = pd.read_csv(r'отправить/3 лучших/submissions_compositions0.csv', index_col='client_id')\n",
    "best_2 = pd.read_csv(r'отправить/3 лучших/submissions_compositions_102.csv', index_col='client_id')\n",
    "best_3 = pd.read_csv(r'отправить/3 лучших/submissions_compositions_111.csv', index_col='client_id')\n",
    "best_df = pd.concat([best_1, best_2, best_3], join='outer', axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 решения с лучшим паблик скором запускаю как признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['Predict_1'] = best_1\n",
    "df_clients['Predict_2'] = best_2\n",
    "df_clients['Predict_3'] = best_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>first_redeem_unixtime</th>\n",
       "      <th>first_issue_unixtime</th>\n",
       "      <th>issue_redeem_delay</th>\n",
       "      <th>total_trans_count</th>\n",
       "      <th>last_month_trans_count</th>\n",
       "      <th>regular_points_received_sum_all</th>\n",
       "      <th>express_points_received_sum_all</th>\n",
       "      <th>regular_points_spent_sum_all</th>\n",
       "      <th>express_points_spent_sum_all</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_qty_items</th>\n",
       "      <th>norm_min_purch_sum</th>\n",
       "      <th>possible_pension</th>\n",
       "      <th>possible_worker</th>\n",
       "      <th>possible_hard_worker</th>\n",
       "      <th>shops_for_weekend</th>\n",
       "      <th>shops_for_week</th>\n",
       "      <th>Predict_1</th>\n",
       "      <th>Predict_2</th>\n",
       "      <th>Predict_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000012768d</th>\n",
       "      <td>45</td>\n",
       "      <td>1.501937e+09</td>\n",
       "      <td>1.515083e+09</td>\n",
       "      <td>-13146559.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>6.037871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000036f903</th>\n",
       "      <td>72</td>\n",
       "      <td>1.491822e+09</td>\n",
       "      <td>1.492940e+09</td>\n",
       "      <td>-1118613.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>54.9</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.087596</td>\n",
       "      <td>4.060443</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000048b7a6</th>\n",
       "      <td>68</td>\n",
       "      <td>1.544870e+09</td>\n",
       "      <td>6.311449e+08</td>\n",
       "      <td>913725130.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>4.727388</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015392</td>\n",
       "      <td>-0.027889</td>\n",
       "      <td>-0.009250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000073194a</th>\n",
       "      <td>60</td>\n",
       "      <td>1.495533e+09</td>\n",
       "      <td>1.511511e+09</td>\n",
       "      <td>-15978107.0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>74.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.873975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039149</td>\n",
       "      <td>0.034640</td>\n",
       "      <td>0.037661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007c7133</th>\n",
       "      <td>67</td>\n",
       "      <td>1.495459e+09</td>\n",
       "      <td>1.546266e+09</td>\n",
       "      <td>-50806825.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>56.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.418841</td>\n",
       "      <td>5.765191</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>0.044241</td>\n",
       "      <td>0.054794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffece623e</th>\n",
       "      <td>67</td>\n",
       "      <td>1.526213e+09</td>\n",
       "      <td>1.545927e+09</td>\n",
       "      <td>-19713429.0</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039210</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.050383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff3dfff8</th>\n",
       "      <td>56</td>\n",
       "      <td>1.541078e+09</td>\n",
       "      <td>1.544193e+09</td>\n",
       "      <td>-3115615.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>117.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.564348</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044752</td>\n",
       "      <td>0.040587</td>\n",
       "      <td>0.049524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffaab9da</th>\n",
       "      <td>23</td>\n",
       "      <td>1.503676e+09</td>\n",
       "      <td>1.512745e+09</td>\n",
       "      <td>-9068995.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>5.398163</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.014677</td>\n",
       "      <td>0.063253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffeb5619</th>\n",
       "      <td>62</td>\n",
       "      <td>1.512470e+09</td>\n",
       "      <td>6.311449e+08</td>\n",
       "      <td>881324727.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>117.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.595120</td>\n",
       "      <td>5.375278</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.008957</td>\n",
       "      <td>-0.024093</td>\n",
       "      <td>0.004311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffff6ce77</th>\n",
       "      <td>42</td>\n",
       "      <td>1.501781e+09</td>\n",
       "      <td>1.503755e+09</td>\n",
       "      <td>-1973789.0</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>221.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-302.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.313206</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400162 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  first_redeem_unixtime  first_issue_unixtime  \\\n",
       "client_id                                                      \n",
       "000012768d   45           1.501937e+09          1.515083e+09   \n",
       "000036f903   72           1.491822e+09          1.492940e+09   \n",
       "000048b7a6   68           1.544870e+09          6.311449e+08   \n",
       "000073194a   60           1.495533e+09          1.511511e+09   \n",
       "00007c7133   67           1.495459e+09          1.546266e+09   \n",
       "...         ...                    ...                   ...   \n",
       "fffece623e   67           1.526213e+09          1.545927e+09   \n",
       "ffff3dfff8   56           1.541078e+09          1.544193e+09   \n",
       "ffffaab9da   23           1.503676e+09          1.512745e+09   \n",
       "ffffeb5619   62           1.512470e+09          6.311449e+08   \n",
       "fffff6ce77   42           1.501781e+09          1.503755e+09   \n",
       "\n",
       "            issue_redeem_delay  total_trans_count  last_month_trans_count  \\\n",
       "client_id                                                                   \n",
       "000012768d         -13146559.0                  4                       2   \n",
       "000036f903          -1118613.0                 32                       8   \n",
       "000048b7a6         913725130.0                  8                       1   \n",
       "000073194a         -15978107.0                 17                       6   \n",
       "00007c7133         -50806825.0                 11                       1   \n",
       "...                        ...                ...                     ...   \n",
       "fffece623e         -19713429.0                 24                       5   \n",
       "ffff3dfff8          -3115615.0                 17                       5   \n",
       "ffffaab9da          -9068995.0                  7                       2   \n",
       "ffffeb5619         881324727.0                  7                       6   \n",
       "fffff6ce77          -1973789.0                 32                      10   \n",
       "\n",
       "            regular_points_received_sum_all  express_points_received_sum_all  \\\n",
       "client_id                                                                      \n",
       "000012768d                             25.7                              0.0   \n",
       "000036f903                             54.9                             60.0   \n",
       "000048b7a6                             26.5                              0.0   \n",
       "000073194a                             74.9                              0.0   \n",
       "00007c7133                             56.6                              0.0   \n",
       "...                                     ...                              ...   \n",
       "fffece623e                             38.4                              0.0   \n",
       "ffff3dfff8                            117.9                              0.0   \n",
       "ffffaab9da                             34.0                              0.0   \n",
       "ffffeb5619                            117.5                              0.0   \n",
       "fffff6ce77                            221.3                              0.0   \n",
       "\n",
       "            regular_points_spent_sum_all  express_points_spent_sum_all  ...  \\\n",
       "client_id                                                               ...   \n",
       "000012768d                           0.0                           0.0  ...   \n",
       "000036f903                           0.0                           0.0  ...   \n",
       "000048b7a6                           0.0                           0.0  ...   \n",
       "000073194a                         -96.0                           0.0  ...   \n",
       "00007c7133                        -240.0                           0.0  ...   \n",
       "...                                  ...                           ...  ...   \n",
       "fffece623e                         -68.0                           0.0  ...   \n",
       "ffff3dfff8                        -182.0                           0.0  ...   \n",
       "ffffaab9da                          -6.0                           0.0  ...   \n",
       "ffffeb5619                           0.0                           0.0  ...   \n",
       "fffff6ce77                        -302.0                           0.0  ...   \n",
       "\n",
       "            norm_qty_items  norm_min_purch_sum  possible_pension  \\\n",
       "client_id                                                          \n",
       "000012768d        3.951244            6.037871                 0   \n",
       "000036f903        5.087596            4.060443                 1   \n",
       "000048b7a6        4.025352            4.727388                 1   \n",
       "000073194a        4.406719            4.873975                 0   \n",
       "00007c7133        4.418841            5.765191                 1   \n",
       "...                    ...                 ...               ...   \n",
       "fffece623e        4.762174            3.367296                 1   \n",
       "ffff3dfff8        4.564348            2.708050                 0   \n",
       "ffffaab9da        4.094345            5.398163                 0   \n",
       "ffffeb5619        4.595120            5.375278                 1   \n",
       "fffff6ce77        5.313206            3.295837                 0   \n",
       "\n",
       "            possible_worker  possible_hard_worker  shops_for_weekend  \\\n",
       "client_id                                                              \n",
       "000012768d                0                     0                  0   \n",
       "000036f903                0                     0                  1   \n",
       "000048b7a6                0                     0                  0   \n",
       "000073194a                0                     0                  1   \n",
       "00007c7133                1                     0                  0   \n",
       "...                     ...                   ...                ...   \n",
       "fffece623e                1                     0                  1   \n",
       "ffff3dfff8                1                     0                  1   \n",
       "ffffaab9da                1                     0                  0   \n",
       "ffffeb5619                1                     0                  1   \n",
       "fffff6ce77                1                     1                  1   \n",
       "\n",
       "            shops_for_week  Predict_1  Predict_2  Predict_3  \n",
       "client_id                                                    \n",
       "000012768d               0   0.000000   0.000000   0.000000  \n",
       "000036f903               0   0.000000   0.000000   0.000000  \n",
       "000048b7a6               0  -0.015392  -0.027889  -0.009250  \n",
       "000073194a               0   0.039149   0.034640   0.037661  \n",
       "00007c7133               0   0.046945   0.044241   0.054794  \n",
       "...                    ...        ...        ...        ...  \n",
       "fffece623e               0   0.039210   0.027638   0.050383  \n",
       "ffff3dfff8               0   0.044752   0.040587   0.049524  \n",
       "ffffaab9da               0   0.022525   0.014677   0.063253  \n",
       "ffffeb5619               0  -0.008957  -0.024093   0.004311  \n",
       "fffff6ce77               0   0.000000   0.000000   0.000000  \n",
       "\n",
       "[400162 rows x 97 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = features + ['Predict_1','Predict_2','Predict_3']\n",
    "df_clients[features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание по 2 конкурентным моделям, разным для таргета и тритмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/2 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean0101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████▎                            | 1/3 [05:12<10:24, 312.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean0112.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 2/3 [08:05<04:30, 270.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean0120.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 3/3 [13:37<00:00, 288.81s/it]\n",
      " 33%|██████████████▎                            | 1/3 [13:37<27:14, 817.12s/it]\n",
      "\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean1201.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████▎                            | 1/3 [03:56<07:52, 236.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean1212.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 2/3 [05:57<03:21, 201.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean1220.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 3/3 [10:32<00:00, 223.67s/it]\n",
      " 67%|████████████████████████████▋              | 2/3 [24:09<12:41, 761.66s/it]\n",
      "\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████▎                            | 1/3 [05:53<11:46, 353.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 2/3 [09:37<05:14, 314.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_mean2020.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 3/3 [16:14<00:00, 339.44s/it]\n",
      " 50%|█████████████████████                     | 1/2 [40:24<40:24, 2424.30s/it]\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min0101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████▎                            | 1/3 [05:16<10:32, 316.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min0112.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 2/3 [08:23<04:37, 277.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min0120.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 3/3 [13:51<00:00, 292.55s/it]\n",
      " 33%|██████████████▎                            | 1/3 [13:51<27:42, 831.30s/it]\n",
      "\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min1201.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████▎                            | 1/3 [03:53<07:47, 233.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min1212.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 2/3 [05:54<03:19, 199.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min1220.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 3/3 [10:31<00:00, 223.10s/it]\n",
      " 67%|████████████████████████████▋              | 2/3 [24:23<12:51, 771.47s/it]\n",
      "\n",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████▎                            | 1/3 [05:37<11:14, 337.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 2/3 [09:31<05:06, 306.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_min2020.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 3/3 [16:00<00:00, 331.24s/it]\n",
      "100%|████████████████████████████████████████| 2/2 [1:20:48<00:00, 2424.19s/it]\n"
     ]
    }
   ],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "X_train=df_clients[features].loc[indices_learn, :].fillna(0).values\n",
    "treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_train=df_train.loc[indices_learn, 'target'].values\n",
    "X_test=df_clients[features].loc[indices_valid, :].fillna(0).values\n",
    "X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "\n",
    "\n",
    "models_treatment = [GradientBoostingClassifier(learning_rate=0.11000000000000001, max_depth=3, min_samples_leaf=1, n_estimators=170),\n",
    "                    lgbm.LGBMClassifier(learning_rate = 0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                    XGBClassifier(eta=0.01, max_depth=4, updater='grow_colmaker')]\n",
    "\n",
    "models_control = [GradientBoostingClassifier(learning_rate=0.05, max_depth=3, min_samples_leaf=1, n_estimators=240),\n",
    "                 lgbm.LGBMClassifier(learning_rate=0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                  XGBClassifier(eta=0.01, max_depth=4)]\n",
    "\n",
    "for element in tqdm(['mean', 'min']): #'max_abs', 'max' показали результат хуже\n",
    "    for i in tqdm(range(0, 3)):\n",
    "        for j in tqdm(range(0,3)):\n",
    "\n",
    "            test_uplift = uplift_fit_predict_2_concurrent_models(\n",
    "                model_treatment_1 = models_treatment[i],\n",
    "                model_treatment_2 = models_treatment[(i+1)%3], \n",
    "                model_control_1 = models_control[j], \n",
    "                model_control_2 = models_control[(j+1)%3],\n",
    "                metric = element,\n",
    "                X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "                treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "                target_train=df_train.loc[indices_train, 'target'].values,\n",
    "                X_test=df_clients[features].loc[indices_test, :].fillna(0).values)\n",
    "\n",
    "            df_submission = pd.DataFrame(data=test_uplift.values, index=df_test.index)\n",
    "            df_submission.columns = ['uplift']\n",
    "            df_submission.to_csv(r'отправить/0/submission_'+element+str(i)+str((i+1)%3)+str(j)+str((j+1)%3)+'.csv')\n",
    "            print('submission_'+element+str(i)+str((i+1)%3)+str(j)+str((j+1)%3)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
