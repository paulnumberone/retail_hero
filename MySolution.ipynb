{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uplift\n",
    "## Целиком ноутбук будет выполняться больше 2 недель, ГДЕ пропускать ОТМЕЧЕНО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import lightgbm as lgbm\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm \n",
    "\n",
    "start=datetime.datetime.now()\n",
    "\n",
    "def uplift_fit_predict(model, X_train, treatment_train, target_train, X_test):\n",
    "    \"\"\"\n",
    "    Реализация простого способа построения uplift-модели.\n",
    "    \n",
    "    Обучаем два одинаковых бинарных классификатора, которые оценивают вероятность target для клиента:\n",
    "    1. с которым была произведена коммуникация (treatment=1)\n",
    "    2. с которым не было коммуникации (treatment=0)\n",
    "    \n",
    "    В качестве оценки uplift для нового клиента берется разница оценок вероятностей:\n",
    "    Predicted Uplift = P(target|treatment=1) - P(target|treatment=0)\n",
    "    \"\"\"\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "    model_treatment = clone(model).fit(X_treatment, y_treatment)\n",
    "    model_control = clone(model).fit(X_control, y_control)\n",
    "    predict_treatment = model_treatment.predict_proba(X_test)[:, 1]\n",
    "    predict_control = model_control.predict_proba(X_test)[:, 1]\n",
    "    predict_uplift = predict_treatment - predict_control\n",
    "    return predict_uplift\n",
    "\n",
    "def uplift_fit_predict_2_models(model_treatment, model_control, X_train, treatment_train, target_train, X_test):\n",
    "    \"\"\"\n",
    "    Обучаем два разных бинарных классификатора, которые оценивают вероятность target для клиента:\n",
    "    1. с которым была произведена коммуникация (treatment=1)\n",
    "    2. с которым не было коммуникации (treatment=0)\n",
    "\n",
    "    \"\"\"\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "    model_treatment.fit(X_treatment, y_treatment)\n",
    "    model_control.fit(X_control, y_control)\n",
    "    predict_treatment = model_treatment.predict_proba(X_test)[:, 1]\n",
    "    predict_control = model_control.predict_proba(X_test)[:, 1]\n",
    "    predict_uplift = predict_treatment - predict_control\n",
    "    return predict_uplift\n",
    "\n",
    "def max_absolute_value_2(dataframe):\n",
    "    \"\"\"\n",
    "    Выбор из двух серий того значения с одинаковым индексом, которое больше по модулю\n",
    "\n",
    "    \"\"\"\n",
    "    buff = []\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "        if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 0]):\n",
    "            buff.append(dataframe.iloc[i, 1])\n",
    "        else:\n",
    "            buff.append(dataframe.iloc[i, 0])\n",
    "    return pd.DataFrame({'uplift': list(buff)}) \n",
    "\n",
    "def max_absolute_value_3(dataframe):\n",
    "    \"\"\"\n",
    "    Выбор из трех серий того значения с одинаковым индексом, которое больше по модулю\n",
    "\n",
    "    \"\"\"\n",
    "    buff = []\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "        if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 0]):\n",
    "            if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 2]):\n",
    "                buff.append(dataframe.iloc[i, 1])\n",
    "            else:\n",
    "                buff.append(dataframe.iloc[i, 2])\n",
    "        else:\n",
    "            if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 2]):\n",
    "                buff.append(dataframe.iloc[i, 0])\n",
    "            else:\n",
    "                buff.append(dataframe.iloc[i, 2])\n",
    "    return buff\n",
    "\n",
    "def max_absolute_value_4(dataframe):\n",
    "    \"\"\"\n",
    "    Выбор из 4ч серий того значения с одинаковым индексом, которое больше по модулю\n",
    "\n",
    "    \"\"\"\n",
    "    buff = []\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "        if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 0]):\n",
    "            if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 2]):\n",
    "                if abs(dataframe.iloc[i, 1]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 1])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "            else:\n",
    "                if abs(dataframe.iloc[i, 2]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 2])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "        else:\n",
    "            if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 2]):\n",
    "                if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 0])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "            else:\n",
    "                if abs(dataframe.iloc[i, 0]) > abs(dataframe.iloc[i, 3]):\n",
    "                    buff.append(dataframe.iloc[i, 2])\n",
    "                else:\n",
    "                    buff.append(dataframe.iloc[i, 3])\n",
    "    return buff\n",
    "\n",
    "def uplift_fit_predict_2_concurrent_models(model_treatment_1, model_treatment_2, model_control_1, model_control_2, metric,\n",
    "                                           X_train, treatment_train, target_train, X_test):\n",
    "    \"\"\"\n",
    "    Обучение для каждой группы клиентов по 2 классификатора, предсказание которых определяется по метрике:\n",
    "    mean - из двух предсказаний построчно выбирается их среднее\n",
    "    max - из двух предсказаний построчно выбирается их максимум\n",
    "    min - из двух предсказаний построчно выбирается их минимум\n",
    "    max_abs - из двух предсказаний построчно выбирается то, которое больше по модулю, с преним знаком\n",
    "\n",
    "    \"\"\"\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "    model_treatment_1.fit(X_treatment, y_treatment)\n",
    "    model_treatment_2.fit(X_treatment, y_treatment)\n",
    "    model_control_1.fit(X_treatment, y_treatment)\n",
    "    model_control_2.fit(X_control, y_control)\n",
    "    \n",
    "    if metric == 'mean':\n",
    "        predict_treatment = pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1).mean(axis=1)\n",
    "\n",
    "        predict_control = pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1).mean(axis=1)\n",
    "    if metric == 'max':\n",
    "        predict_treatment = pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1).max(axis=1)\n",
    "\n",
    "        predict_control = pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1).max(axis=1)\n",
    "        \n",
    "    if metric == 'min':\n",
    "        predict_treatment = pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1).min(axis=1)\n",
    "\n",
    "        predict_control = pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1).min(axis=1)\n",
    "        \n",
    "    if metric == 'max_abs':\n",
    "        predict_treatment = max_absolute_value_2(pd.concat([pd.DataFrame(model_treatment_1.predict_proba(X_test)[:, 1]), \n",
    "                                       pd.DataFrame(model_treatment_2.predict_proba(X_test)[:, 1])],\n",
    "                                      join='outer', axis = 1))\n",
    "\n",
    "        predict_control = max_absolute_value_2(pd.concat([pd.DataFrame(model_control_1.predict_proba(X_test)[:, 1]), \n",
    "                                     pd.DataFrame(model_control_2.predict_proba(X_test)[:, 1])],\n",
    "                                    join='outer', axis = 1))\n",
    "    \n",
    "    predict_uplift = predict_treatment - predict_control\n",
    "    return predict_uplift\n",
    "\n",
    "def uplift_score(prediction, treatment, target, rate=0.3):\n",
    "    \"\"\"\n",
    "    Подсчет Uplift Score\n",
    "    \"\"\"\n",
    "    order = np.argsort(-prediction)\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "    score = treatment_p - control_p\n",
    "    return score\n",
    "\n",
    "def important_feats_for_model(model, features):\n",
    "    \"\"\"\n",
    "    Определение наиболее важных признаков для модели\n",
    "    \"\"\"\n",
    "    X_train=features.loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_treatment, y_treatment = X_train[treatment_train == 1], target_train[treatment_train == 1]\n",
    "    X_control, y_control = X_train[treatment_train == 0], target_train[treatment_train == 0]\n",
    "    model_treatment = clone(model).fit(X_treatment, y_treatment)\n",
    "    model_control = clone(model).fit(X_control, y_control)\n",
    "    print(pd.DataFrame(model_treatment.feature_importances_, features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных\n",
    "df_train = pd.read_csv('data/uplift_train.csv', index_col='client_id')\n",
    "df_test = pd.read_csv('data/uplift_test.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МОЖНО ПРОПУСТИТЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляю данные о покупках и товарах\n",
    "df_products = pd.read_csv('data/products.csv')\n",
    "df_purchases = pd.read_csv('data/purchases.csv')\n",
    "df_clients = pd.read_csv('data/clients.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение признаков\n",
    "df_clients['first_redeem_unixtime'] = pd.Series([y.timestamp() for y in \n",
    "                                                 [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "                                                  df_clients['first_issue_date']]]).values\n",
    "df_clients['first_issue_unixtime'] = pd.Series([x.timestamp() for x in \n",
    "                                                [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "                                                 df_clients['first_redeem_date'].fillna('1990-01-01 01:01:01')]]).values\n",
    "df_clients['issue_redeem_delay'] = df_clients['first_redeem_unixtime'] - df_clients['first_issue_unixtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = ['regular_points_received', 'express_points_received','regular_points_spent', 'express_points_spent', 'purchase_sum','store_id']\n",
    "all_hist = df_purchases.groupby(['client_id','transaction_id'])[last_cols].last()\n",
    "last_month = df_purchases[df_purchases['transaction_datetime'] > '2019-02-18'].groupby(['client_id','transaction_id'])[last_cols].last()\n",
    "\n",
    "features =  pd.concat([all_hist.groupby('client_id')['purchase_sum'].count(),\n",
    "                       last_month.groupby('client_id')['purchase_sum'].count(),\n",
    "                       all_hist.groupby('client_id').sum(),\n",
    "                       all_hist.groupby('client_id')[['store_id']].nunique(),\n",
    "                       last_month.groupby('client_id').sum(),\n",
    "                       last_month.groupby('client_id')[['store_id']].nunique(),\n",
    "                      ],axis = 1)\n",
    "features.columns = ['total_trans_count','last_month_trans_count']+list(c+\"_sum_all\" for c in last_cols)+list(c+\"_sum_last_month\" for c in last_cols)\n",
    "df_clients[list(features.columns)] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_clients = pd.read_csv('data/clients.csv', index_col='client_id', parse_dates=['first_issue_date','first_redeem_date'])\n",
    "df_clients['first_issue_date_weekday'] = temp_df_clients['first_issue_date'].dt.weekday\n",
    "df_clients['first_redeem_date_weekday'] = temp_df_clients['first_redeem_date'].dt.weekday\n",
    "df_clients['first_issue_date_hour'] = temp_df_clients['first_issue_date'].dt.hour\n",
    "df_clients['first_redeem_date_hour'] = temp_df_clients['first_redeem_date'].dt.hour\n",
    "del temp_df_clients, all_hist, last_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Часы, в которые клиенты совершали покупки\n",
    "df_purchases['transaction_hour'] = pd.Series([x.hour for x in \n",
    "           [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "            df_purchases['transaction_datetime']]])\n",
    "df_temp = df_purchases[['client_id','transaction_hour']].groupby(['client_id','transaction_hour']).nunique()\n",
    "users, hours, nums = [], [], []\n",
    "for i in range(0, len(df_temp.index)):\n",
    "    users.append(df_temp.index[i][0])\n",
    "    hours.append(df_temp.index[i][1])\n",
    "    nums.append(df_temp.values[i][0])\n",
    "boughts_by_hours = pd.DataFrame(data=zip(users, hours, nums), columns=['client_id', 'hours', 'nums'])\n",
    "boughts_by_hours_dum = pd.get_dummies(boughts_by_hours['hours'])\n",
    "boughts_by_hours_dum['client_id'] = boughts_by_hours['client_id']\n",
    "boughts_by_hours_dum = boughts_by_hours_dum.groupby(['client_id']).sum().astype(int)\n",
    "boughts_by_hours_dum.columns = ['0h', '1h', '2h', '3h', '4h', '5h', '6h', '7h', '8h', '9h', '10h', '11h', '12h', '13h', \n",
    "                                '14h', '15h', '16h', '17h', '18h', '19h', '20h', '21h', '22h', '23h']\n",
    "df_clients = pd.concat([df_clients, boughts_by_hours_dum], join='outer', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дни, в которые клиенты совершали покупки\n",
    "\n",
    "df_purchases['transaction_day'] = pd.Series([x.weekday() for x in \n",
    "           [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in \n",
    "            df_purchases['transaction_datetime']]])\n",
    "df_temp = df_purchases[['client_id','transaction_day']].groupby(['client_id','transaction_day']).nunique()\n",
    "users, days, nums = [], [], []\n",
    "for i in range(0, len(df_temp.index)):\n",
    "    users.append(df_temp.index[i][0])\n",
    "    days.append(df_temp.index[i][1])\n",
    "    nums.append(df_temp.values[i][0])\n",
    "boughts_by_days = pd.DataFrame(data=zip(users, days, nums), columns=['client_id', 'days', 'nums'])\n",
    "boughts_by_days_dum = pd.get_dummies(boughts_by_days['days'])\n",
    "boughts_by_days_dum['client_id'] = boughts_by_days['client_id']\n",
    "boughts_by_days_dum = boughts_by_days_dum.groupby(['client_id']).sum().astype('category')\n",
    "boughts_by_days_dum.columns = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "df_clients = pd.concat([df_clients, boughts_by_days_dum], join='outer', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del users, hours, days, nums, boughts_by_hours, boughts_by_hours_dum, boughts_by_days, boughts_by_days_dum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конверт дат в юникс\n",
    "dt_list = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in df_purchases['transaction_datetime']]\n",
    "df_purchases['transaction_datetime'] = pd.Series([y.timestamp() for y in dt_list]).values\n",
    "del dt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пол в категорию\n",
    "df_clients['gender_M'] = (df_clients['gender'] == 'M').astype('category')\n",
    "df_clients['gender_F'] = (df_clients['gender'] == 'F').astype('category')\n",
    "df_clients['gender_U'] = (df_clients['gender'] == 'U').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наличие алкоголя в чеке\n",
    "df_clients['has_alco'] = (pd.merge(df_purchases, df_products[['product_id', 'is_alcohol']], on = 'product_id', \n",
    "                                   sort=False).groupby(['client_id']).is_alcohol.sum()>0).astype('category')\n",
    "# Средний чек покупателя\n",
    "df_clients['avg_check'] = df_clients['amount'] / df_clients['n_purchases']\n",
    "# Периодичность покупок\n",
    "df_clients['purch_freq'] = (df_purchases.groupby(['client_id']).transaction_datetime.max() - \n",
    "                            df_purchases.groupby(['client_id']).transaction_datetime.min())/df_clients['n_purchases']\n",
    "# Наличие алкоголя и товаров своей марки в чеке\n",
    "df_clients['has_alco_and_own'] = ((pd.merge(df_purchases, df_products[['product_id', 'is_alcohol']], on = 'product_id', \n",
    "                                   sort=False).groupby(['client_id']).is_alcohol.sum()>0) & \n",
    "                                 (pd.merge(df_purchases, df_products[['product_id', 'is_own_trademark']], on = 'product_id', \n",
    "                                   sort=False).groupby(['client_id']).is_own_trademark.mean()>0.5)).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возраст покупателя (сам по себе возраст как число не имеет важности)\n",
    "df_clients['age_error'] = ((df_clients['age'] <= 10) & (df_clients['age'] > 99)).astype('category')\n",
    "df_clients['age_1'] = ((df_clients['age'] >10) & (df_clients['age'] <= 18)).astype('category')\n",
    "df_clients['age_2'] = ((df_clients['age'] >18) & (df_clients['age'] <= 25)).astype('category')\n",
    "df_clients['age_3'] = ((df_clients['age'] >25) & (df_clients['age'] <= 35)).astype('category')\n",
    "df_clients['age_4'] = ((df_clients['age'] >35) & (df_clients['age'] <= 45)).astype('category')\n",
    "df_clients['age_5'] = ((df_clients['age'] >45) & (df_clients['age'] <= 55)).astype('category')\n",
    "df_clients['age_6'] = ((df_clients['age'] >55) & (df_clients['age'] <= 99)).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['regular_points_received'] = df_purchases[['client_id', 'transaction_id', \n",
    "                                                      'regular_points_received']].groupby(['client_id']).sum()\n",
    "df_clients['regular_points_spent'] = df_purchases[['client_id', 'transaction_id', \n",
    "                                                   'regular_points_spent']].groupby(['client_id']).sum().replace(0, 1)\n",
    "df_clients['regular_points_balance'] = df_clients['regular_points_received'] + df_clients['regular_points_spent']\n",
    "df_clients['bonuses'] = round(df_clients['amount']/df_clients['regular_points_spent'], 2)\n",
    "df_clients['max_product_quantity'] = df_purchases[['client_id', 'product_quantity']].groupby(['client_id']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['express_points_received'] = df_purchases[['client_id', 'transaction_id', \n",
    "                                                      'express_points_received']].groupby(['client_id']).sum()\n",
    "df_clients['express_points_spent'] = df_purchases[['client_id', 'transaction_id', \n",
    "                                                   'express_points_spent']].groupby(['client_id']).sum().replace(0, 1)\n",
    "df_clients['express_points_balance'] = df_clients['express_points_received'] + df_clients['express_points_spent']\n",
    "df_clients['express_bonuses'] = round(df_clients['express_points_spent']/df_clients['amount'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможные категории\n",
    "\n",
    "df_clients['possible_hard_worker']=(((df_clients['1h']+df_clients['2h']+df_clients['3h']+df_clients['23h']+\n",
    "                                      df_clients['22h']+df_clients['21h'])>0) & (df_clients['age']>20) & \n",
    "                                    (df_clients['has_alco'].astype(int))>0).astype('category')\n",
    "df_clients['shops_for_weekend'] = (((df_clients['fri'].astype(int)+df_clients['sat'].astype(int))>1) & \n",
    "                                   df_clients['has_alco'].astype(int)>0).astype('category')\n",
    "df_clients['shops_for_week'] = ((df_clients['fri'].astype(int)+df_clients['sat'].astype(int)+\n",
    "                                 df_clients['sun'].astype(int))==1).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормирую время\n",
    "df_clients['norm_first_issue_unixtime'] = df_clients['first_issue_unixtime']/df_clients['first_issue_unixtime'].values.mean()\n",
    "df_clients['norm_first_redeem_unixtime'] = df_clients['first_redeem_unixtime']/df_clients['first_redeem_unixtime'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потрачено клиентом нормированное\n",
    "df_clients['norm_amount'] = df_clients['amount']/df_clients['amount'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество магазинов, посещенных клиентом и чек нормированные\n",
    "df_clients['norm_n_shops'] = np.log(df_clients['n_shops'])\n",
    "df_clients['norm_avg_check'] = np.log(df_clients['avg_check'])\n",
    "df_clients['norm_qty_items'] = np.log(df_clients['qty_items'])\n",
    "df_clients['norm_min_purch_sum'] = np.log(df_clients['min_purch_sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Некоторые фичи удалил, конкурс-то еще идет =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перемножение и сложение фичей (выбрал вручную) +0,03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_operate = ['total_trans_count', 'last_month_trans_count', 'regular_points_received_sum_all', \n",
    "                       'express_points_received_sum_all', 'regular_points_spent_sum_all', 'express_points_spent_sum_all', \n",
    "                       'purchase_sum_sum_all', 'purchase_sum_sum_all', 'regular_points_received_sum_last_month', \n",
    "                       'express_points_received_sum_last_month', 'regular_points_spent_sum_last_month', \n",
    "                       'express_points_spent_sum_last_month', 'purchase_sum_sum_last_month', 'qty_items', \n",
    "                       'amount', 'n_purchases', 'max_purch_sum', 'min_purch_sum', 'avg_check', 'sums_relation', \n",
    "                       'regular_points_received', 'regular_points_spent', 'regular_points_balance', 'bonuses', \n",
    "                       'max_product_quantity', 'express_points_received', 'express_points_spent', 'express_points_balance',\n",
    "                       'express_bonuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(features_to_operate)):\n",
    "    for j in range(0, len(features_to_operate)):\n",
    "        if i!=j:\n",
    "            df_clients[features_to_operate[i] + '_sum_' + features_to_operate[j]] = (df_clients[features_to_operate[i]] + df_clients[features_to_operate[j]])\n",
    "            df_clients[features_to_operate[i] + '_multi_' + features_to_operate[j]] = (df_clients[features_to_operate[i]] * df_clients[features_to_operate[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт на случай если все рухнет\n",
    "df_clients.to_csv('D:/df_clients.csv')\n",
    "datetime.datetime.now()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Освобождаю память\n",
    "del df_products\n",
    "del df_purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДО СЮДА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение после обвала\n",
    "df_clients = pd.read_csv('D:/df_clients.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалить нормированные признаки\n",
    "# features = list(df_clients.columns[2:-7].values) # удаляю 'first_issue_date', 'first_redeem_date'\n",
    "features = list(df_clients.columns[2:].values) # удаляю 'first_issue_date', 'first_redeem_date'\n",
    "#df_clients[features].fillna(0)\n",
    "features.remove('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбираем важные для модели фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отбираем важные для модели фичи\n",
    "import feature_selector\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "# Для treatment_flg\n",
    "fs = FeatureSelector(df_clients[features].loc[indices_learn, :].fillna(0), df_train.loc[indices_learn, 'treatment_flg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# признаки, имеющие более 60% пропусков\n",
    "fs.identify_missing(missing_threshold = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# доли незаполненных значений для каждого параметра\n",
    "fs.missing_stats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# признаки для удаления\n",
    "fs.ops['missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_collinear(correlation_threshold = 0.99)\n",
    "\n",
    "# признаки коллинеарные\n",
    "fs.record_collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "fs.plot_collinear(plot_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для target\n",
    "fs = FeatureSelector(df_clients[features].loc[indices_learn, :].fillna(0), df_train.loc[indices_learn, 'target'])\n",
    "# Oтбор признаков с нулевой важностью для target\n",
    "fs.identify_zero_importance(task = 'classification', \n",
    "                            eval_metric = 'auc', \n",
    "                            n_iterations = 10)\n",
    "\n",
    "# Hормализованные показатели важности plot_n самых значимых признаков\n",
    "fs.plot_feature_importances(threshold = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = list(fs.ops['zero_importance'])\n",
    "to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in to_delete:\n",
    "    features.remove(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследуем фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МОЖНО ПРОПУСТИТЬ - аномалий во взаимосвязях не выявлено"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(features)):\n",
    "    for j in range(0, len(features)):\n",
    "        plt.rcParams['figure.figsize'] = (10,8)\n",
    "        plt.scatter(df_clients[features[i]].loc[indices_train[:200000]], df_clients[features[j]].loc[indices_train[:200000]],\n",
    "               c='green', s=3)\n",
    "        plt.scatter(df_clients[features[i]].loc[indices_test[:200000]], df_clients[features[j]].loc[indices_test[:200000]],\n",
    "               c='red', s=2)\n",
    "        plt.ylabel(features[i])\n",
    "        plt.xlabel(features[j])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "for i in range(4, len(features)):\n",
    "    plt.rcParams['figure.figsize'] = (10,8)\n",
    "    plt.title('Распределение целевого признака ' + str(features[i]) + 'для групп покупателей')\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(df_clients[features[i]].loc[indices_train], color='green')\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(df_clients[features[i]].loc[indices_test], color='red')\n",
    "    plt.title('Распределение целевого признака ' + str(features[i]) + ' для групп покупателей')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение предсказания одним алгоритмом для 0 и 1\n",
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по подобранным n_estimators = 170, max_depth = 1\n",
    "\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=GradientBoostingClassifier(n_estimators = 170, max_depth = 1),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=GradientBoostingClassifier(n_estimators = 170, max_depth = 1),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission34.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по ADABOOST по подобранным learning_rate = 0.1, n_estimators = 100\n",
    "\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=AdaBoostClassifier(learning_rate = 0.1, n_estimators = 100, n_jobs=-1),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=AdaBoostClassifier(learning_rate = 0.1, n_estimators = 100, n_jobs=-1),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission35.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по LGBM\n",
    "\n",
    "import lightgbm as lgbm\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=lgbm.LGBMClassifier(learning_rate = 0.03, max_depth = 5, num_leaves = 20,\n",
    "             min_data_in_leaf = 3, application = 'binary',\n",
    "             subsample = 0.8, colsample_bytree = 0.8,\n",
    "             reg_alpha = 0.01,data_random_seed = 42,metric = 'binary_logloss',\n",
    "             max_bin = 416,bagging_freq = 3,reg_lambda = 0.01, n_estimators = 1000),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=lgbm.LGBMClassifier(learning_rate = 0.03, max_depth = 5, num_leaves = 20,\n",
    "             min_data_in_leaf = 3, application = 'binary',\n",
    "             subsample = 0.8, colsample_bytree = 0.8,\n",
    "             reg_alpha = 0.01,data_random_seed = 42,metric = 'binary_logloss',\n",
    "             max_bin = 416,bagging_freq = 3,reg_lambda = 0.01, n_estimators = 1000),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission36.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества на валидации по XGBClassifier\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "valid_uplift = uplift_fit_predict(\n",
    "    model=XGBClassifier(),\n",
    "    X_train=df_clients[features].loc[indices_learn, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_learn, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_valid, :].fillna(0).values,\n",
    ")\n",
    "valid_score = uplift_score(\n",
    "    valid_uplift,\n",
    "    treatment=df_train.loc[indices_valid, 'treatment_flg'].values,\n",
    "    target=df_train.loc[indices_valid, 'target'].values,\n",
    ")\n",
    "print('Validation score:', valid_score)\n",
    "\n",
    "# 0.06186994571840043 = 0,0918\n",
    "# 0.06186994571840043 = \n",
    "# 0.06186994571840043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предсказаний для тестовых клиентов\n",
    "\n",
    "test_uplift = uplift_fit_predict(\n",
    "    model=XGBClassifier(),\n",
    "    X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "    treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "    target_train=df_train.loc[indices_train, 'target'].values,\n",
    "    X_test=df_clients[features].loc[indices_test, :].fillna(0).values,\n",
    ")\n",
    "\n",
    "df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "df_submission.to_csv('submission37.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор гиперпараметров для моделей по отдельности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "X_train=df_clients[features].loc[indices_learn, :].fillna(0).values\n",
    "treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_train=df_train.loc[indices_learn, 'target'].values\n",
    "X_test=df_clients[features].loc[indices_valid, :].fillna(0).values\n",
    "X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': range(1, 6), \n",
    "              'tree_method': ['auto', 'exact', 'approx'],# 'hist', 'gpu_hist'],\n",
    "              'updater': ['grow_colmaker', 'distcol', 'grow_histmaker'], # 'grow_local_histmaker', # 'grow_skmaker', 'grow_quantile_histmaker', 'sync', 'refresh'], \n",
    "              'eta': np.arange(0.01, 0.21, 0.02),\n",
    "              'num_parallel_tree': [1, 2, 3]}\n",
    "\n",
    "model=XGBClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 3, verbose=True)\n",
    "model_treatment_XGBC = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_XGBC = clf.fit(X_control, y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_treatment_XGBC.best_params_, model_treatment_XGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_XGBC.best_params_, model_control_XGBC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "X_train=df_clients[features].loc[indices_learn, :].fillna(0).values\n",
    "treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_train=df_train.loc[indices_learn, 'target'].values\n",
    "X_test=df_clients[features].loc[indices_valid, :].fillna(0).values\n",
    "X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': range(150, 250, 10),\n",
    "              'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "              'loss': ['deviance', 'exponential'],\n",
    "              'max_depth': range(1, 4), \n",
    "              'learning_rate': np.arange(0.05, 0.14, 0.02), \n",
    "              'min_samples_leaf':[1, 2]}\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv = 3, n_jobs = -1, verbose=True)\n",
    "model_treatment_gbc = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_gbc = clf.fit(X_control, y_control)\n",
    "# в каггле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_treatment_gbc.best_params_, model_treatment_gbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_control_gbc.best_params_, model_control_gbc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': range(40, 250, 10), \n",
    "              'learning_rate': np.arange(0.01, 0.21, 0.02)}\n",
    "\n",
    "model=AdaBoostClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 3, n_jobs = -1, verbose=True)\n",
    "model_treatment_adaboost = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_adaboost = clf.fit(X_control, y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_treatment_adaboost.best_params_, model_treatment_adaboost.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_adaboost.best_params_, model_control_adaboost.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "parameters = {'max_depth': range(1, 4), \n",
    "              'n_estimators': range(150, 250, 10),\n",
    "              'learning_rate': np.arange(0.01, 0.21, 0.02)}\n",
    "\n",
    "model=lgbm.LGBMClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 3, n_jobs = -1, verbose=True)\n",
    "model_treatment_LGBM = clf.fit(X_treatment, y_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_control_LGBM = clf.fit(X_control, y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_treatment_LGBM.best_params_, model_treatment_LGBM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_control_LGBM.best_params_, model_control_LGBM.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание по 2 моделям, разным для таргета и тритмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start=datetime.datetime.now()\n",
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "\n",
    "models_treatment = [GradientBoostingClassifier(learning_rate=0.11000000000000001, max_depth=3, min_samples_leaf=1, n_estimators=170),\n",
    "                    lgbm.LGBMClassifier(learning_rate = 0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                    AdaBoostClassifier(learning_rate=0.18999999999999997, n_estimators=230),\n",
    "                    XGBClassifier(eta=0.01, max_depth=4)]\n",
    "\n",
    "models_control = [GradientBoostingClassifier(learning_rate=0.05, max_depth=3, min_samples_leaf=1, n_estimators=240),\n",
    "                  lgbm.LGBMClassifier(learning_rate=0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                  AdaBoostClassifier(learning_rate=0.10999999999999997, n_estimators=230),\n",
    "                  XGBClassifier(eta=0.01, max_depth=4)]\n",
    "\n",
    "from tqdm import tqdm \n",
    "i=0\n",
    "for model_treatment in tqdm(models_treatment):\n",
    "    for model_control in tqdm(models_control):\n",
    "        test_uplift = uplift_fit_predict_2_models(model_treatment, model_control,\n",
    "            X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "            treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "            target_train=df_train.loc[indices_train, 'target'].values,\n",
    "            X_test=df_clients[features].loc[indices_test, :].fillna(0).values)\n",
    "        \n",
    "        df_submission = pd.DataFrame({'uplift': test_uplift}, index=df_test.index)\n",
    "        df_submission.to_csv('отправить/3(фичикатегории)/submission3000_'+str(i)+'.csv')\n",
    "        i += 1\n",
    "        \n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Все 16 решений отравлены, выбираю 4 лучших среди всех и комбинирую"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинация из 4 решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "godn_submission1 = pd.read_csv(r'отправить/отправлено/submission1.csv', index_col='client_id')\n",
    "godn_submission13 = pd.read_csv(r'отправить/отправлено/submission13.csv', index_col='client_id')\n",
    "godn_submission2_0 = pd.read_csv(r'отправить/отправлено/submission2_0.csv', index_col='client_id')\n",
    "godn_submission37 = pd.read_csv(r'отправить/отправлено/submission37.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13, godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:4].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:4].min(axis=1)\n",
    "effective_subs['abs_max'] = effective_subs.iloc[:, 0:4].abs().max(axis=1)\n",
    "effective_subs['abs_min'] = effective_subs.iloc[:, 0:4].abs().min(axis=1)\n",
    "for i in range(0, 5):\n",
    "    effective_subs.iloc[:, 4+i].to_csv(r'отправить/отправлено/submissions_compositions'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13, godn_submission2_0], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "effective_subs['abs_max'] = effective_subs.iloc[:, 0:3].abs().max(axis=1)\n",
    "effective_subs['abs_min'] = effective_subs.iloc[:, 0:3].abs().min(axis=1)\n",
    "for i in range(0, 5):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_2'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинации из 3 решений - быстрее было перебрать, чем написать функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_3'+str(i)+'.csv')\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_4'+str(i)+'.csv')\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission13, godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:3].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:3].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 3+i].to_csv(r'отправить/submissions_compositions_5'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинации из 2 решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission13], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_6'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission2_0], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_7'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission1, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_8'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission13, godn_submission2_0], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_9'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission13, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_10'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "effective_subs = pd.concat([godn_submission2_0, godn_submission37], join='outer', axis = 1)\n",
    "effective_subs['mean'] = effective_subs.mean(axis=1)\n",
    "effective_subs['max'] = effective_subs.iloc[:, 0:2].max(axis=1)\n",
    "effective_subs['min'] = effective_subs.iloc[:, 0:2].min(axis=1)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    effective_subs.iloc[:, 2+i].to_csv(r'отправить/submissions_compositions_11'+str(i)+'.csv')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего зашли минимум, максимум и среднее по 4м лучшим решениям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 решения с лучшим паблик скором объединяю по абсолютному значению (скор не побил)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_1 = pd.read_csv(r'отправить/submissions_compositions0.csv', index_col='client_id')\n",
    "best_2 = pd.read_csv(r'отправить/submissions_compositions_102.csv', index_col='client_id')\n",
    "best_3 = pd.read_csv(r'отправить/submissions_compositions_111.csv', index_col='client_id')\n",
    "best_df = pd.concat([best_1, best_2, best_3], join='outer', axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df = pd.DataFrame({'uplift': list(max_absolute_value_3(best_df))}, index=best_1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df.to_csv(r'отправить/best_df.csv')\n",
    "# 0,0939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_1 = pd.read_csv(r'отправить/3 лучших/submissions_compositions0.csv', index_col='client_id')\n",
    "best_2 = pd.read_csv(r'отправить/3 лучших/submissions_compositions_102.csv', index_col='client_id')\n",
    "best_3 = pd.read_csv(r'отправить/3 лучших/submissions_compositions_111.csv', index_col='client_id')\n",
    "best_df = pd.concat([best_1, best_2, best_3], join='outer', axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 решения с лучшим паблик скором запускаю как признаки - не зашло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['Predict_1'] = best_1\n",
    "df_clients['Predict_2'] = best_2\n",
    "df_clients['Predict_3'] = best_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features + ['Predict_1','Predict_2','Predict_3']\n",
    "df_clients[features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДО СЮДА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание по 2 конкурентным моделям, разным для таргета и тритмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices_train = df_train.index\n",
    "indices_test = df_test.index\n",
    "indices_learn, indices_valid = train_test_split(df_train.index, test_size=0.3, random_state=123)\n",
    "X_train=df_clients[features].loc[indices_learn, :].fillna(0).values\n",
    "treatment_train=df_train.loc[indices_learn, 'treatment_flg'].values\n",
    "target_train=df_train.loc[indices_learn, 'target'].values\n",
    "X_test=df_clients[features].loc[indices_valid, :].fillna(0).values\n",
    "X_treatment, y_treatment = X_train[treatment_train == 1, :], target_train[treatment_train == 1]\n",
    "X_control, y_control = X_train[treatment_train == 0, :], target_train[treatment_train == 0]\n",
    "\n",
    "\n",
    "models_treatment = [GradientBoostingClassifier(learning_rate=0.11000000000000001, max_depth=3, min_samples_leaf=1, n_estimators=170),\n",
    "                    lgbm.LGBMClassifier(learning_rate = 0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                    XGBClassifier(eta=0.01, max_depth=4, updater='grow_colmaker')]\n",
    "\n",
    "models_control = [GradientBoostingClassifier(learning_rate=0.05, max_depth=3, min_samples_leaf=1, n_estimators=240),\n",
    "                 lgbm.LGBMClassifier(learning_rate=0.06999999999999999, max_depth=3, n_estimators=180),\n",
    "                  XGBClassifier(eta=0.01, max_depth=4)]\n",
    "\n",
    "for element in tqdm(['mean', 'min']): #'max_abs', 'max' показали результат хуже\n",
    "    for i in tqdm(range(0, 3)):\n",
    "        for j in tqdm(range(0,3)):\n",
    "\n",
    "            test_uplift = uplift_fit_predict_2_concurrent_models(\n",
    "                model_treatment_1 = models_treatment[i],\n",
    "                model_treatment_2 = models_treatment[(i+1)%3], \n",
    "                model_control_1 = models_control[j], \n",
    "                model_control_2 = models_control[(j+1)%3],\n",
    "                metric = element,\n",
    "                X_train=df_clients[features].loc[indices_train, :].fillna(0).values,\n",
    "                treatment_train=df_train.loc[indices_train, 'treatment_flg'].values,\n",
    "                target_train=df_train.loc[indices_train, 'target'].values,\n",
    "                X_test=df_clients[features].loc[indices_test, :].fillna(0).values)\n",
    "\n",
    "            df_submission = pd.DataFrame(data=test_uplift.values, index=df_test.index)\n",
    "            df_submission.columns = ['uplift']\n",
    "            df_submission.to_csv(r'отправить/0/submission_'+element+str(i)+str((i+1)%3)+str(j)+str((j+1)%3)+'.csv')\n",
    "            print('submission_'+element+str(i)+str((i+1)%3)+str(j)+str((j+1)%3)+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
